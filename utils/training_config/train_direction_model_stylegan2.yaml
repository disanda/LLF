model_name: ffhq_k30_f32-c5_9_convs5 # learning eye
k: 30
batch_size: 3
iterations: 1200 #eval = 100
grad_clip_max_norm: null # change to float to activate
optimizer:
  _target_: torch.optim.Adam
  lr: 0.001
  weight_decay: 1e-3  
  # _target_: torch.optim.SGD
  # lr: 0.1
  # weight_decay: 5e-4
  # momentum: 0.1
  # nesterov: true
scheduler:
  _target_: torch.optim.lr_scheduler.MultiStepLR
  milestones: [1000, 5000]
  gamma: 0.2
generator_path: './checkpoint/generators/stylegan2-ffhq-config-f.pt'
#train
tensorboard: True
eval_freq: 120 # epoch = iterations // eval_freq
eval_iters: 120
feed_layers: null
model_load_path: null

kmeans:
  use_kmeans: True
  kmeans_model_path: './checkpoint/kmeans_segmentation/dataset_ffhq_kmeans_k_clusters_9_feature_size_32_samples_128_feature_layer_convs.5_trunc_0.5.pkl' 
  k_th_cluster: 0
  use_mse_loss: False

generator:
  # _target_: generators.BigGANGenerator #BigGAN
  # resolution: 256
  # device: cuda
  # truncation: 0.4
  # class_name: husky
  # feature_layer: generator.layers.0
  _target_: models.generators.stylegan2.stylegan2_wrap.StyleGAN2Generator ##StyleGAN2
  truncation: 0.85
  class_name: ffhq
  use_w: true
  feature_layer: convs.5 # conv1: 4*4 , [convs.0, convs.1] : 8*8,  [convs.2, convs.3] : 16*16, [convs.4, convs.5] : 32*32,  [convs.6, convs.7] : 64*64,

#checkpoint_path:
model:
  size: 512 # StyleGANs: 512  BigGAN256: 128
  _target_: model.NonlinearConditional
  normalize: True
  alpha: 0.1
  depth: 3
  final_ac: False

loss:
  _target_: loss.ContrastiveLoss
  temp: 0.5
  abs: True
  reduce: mean

# version: 1  # logging
# formatters:
#   simple:
#     format: '%(message)s'
#   time:
#     format: '[%(asctime)s]- %(message)s'
# handlers:
#   console:
#     class: logging.StreamHandler
#     formatter: simple
#     stream: ext://sys.stdout
#   file:
#     class: logging.FileHandler
#     formatter: time
#     # relative to the job log directory
#     filename: ${hydra.job.name}.log
# root:
#   level: INFO
#   handlers: [console, file]

# disable_existing_loggers: false

# run: #output
#   dir: outputs/run/${hydra.job.name}/${generator._target_}/${generator.feature_layer}_${generator.class_name}_${k}/${model._target_}_${projector._target_}/${now:%Y-%m-%d}
# sweep:
#   dir: outputs/multirun/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}
#   subdir: ${hydra.job.num}_${hydra.job.override_dirname}
# job:
#   config:
#     override_dirname:
#       exclude_keys:
#         - seed
#         - device
